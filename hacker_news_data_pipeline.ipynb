{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building a data pipeline: Hacker News Social Media Data\n",
    "\n",
    "In this project, we will build our own data pipeline class in Python. This class will be constructed around a directed acyclic graph as the scheduler for it. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a Hacker News (HN) API that returns JSON data of the top stories in 2014. The list of JSON posts can be downloaded from the file called `hn_stories_2014.json`. The JSON file contains a single key `stories`, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "\n",
    "- `created_at`: A timestamp of the story's creation time.\n",
    "- `created_at_i`: A unix epoch timestamp.\n",
    "- `url`: The URL of the story link.\n",
    "- `objectID`: The ID of the story.\n",
    "- `author`: The story's author (username on HN).\n",
    "- `points`: The number of upvotes the story had.\n",
    "- `title`: The headline of the post.\n",
    "- `num_comments`: The number of a comments a post has.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using our `Pipeline` class. The goal will be to find the top 100 keywords of Hacker News posts in 2014.\n",
    "\n",
    "### 1. Building the `Pipeline` class\n",
    "\n",
    "#### 1.1 Constructing the directed acyclic graph (DAG) class\n",
    "\n",
    "The pipeline exhibits the requirements of a DAG. First, there are a set of vertices (tasks) and edges (links between tasks), second, there is a direction of each task, and finally, there are no cycles.\n",
    "\n",
    "The DAG structure is built in a way that naturally creates an efficient ordering of dependent tasks. There is a DAG sorting algorithm for exposing this order that we can take advantage of when scheduling our tasks.\n",
    "\n",
    "Specifically, the algorithm we will implement is called [Kahn's Algorithm](https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm), a famous DAG sorting algorithm. An interesting property about this sorting algorithm is that it can also determine if a graph has a cycle or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DAG:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "            \n",
    "    def in_degrees(self):\n",
    "        self.degrees = {}\n",
    "        for node in self.graph:\n",
    "            if node not in self.degrees:\n",
    "                self.degrees[node] = 0\n",
    "            for pointer in self.graph[node]:\n",
    "                if pointer not in self.degrees:\n",
    "                    self.degrees[pointer] = 1\n",
    "                else:\n",
    "                    self.degrees[pointer] += 1\n",
    "                    \n",
    "    def sort(self):\n",
    "        self.in_degrees()\n",
    "        d = deque()\n",
    "        searched = []\n",
    "        for node in self.degrees:\n",
    "            if self.degrees[node] == 0:\n",
    "                d.append(node)\n",
    "        while d:\n",
    "            node_i = d.popleft()\n",
    "            for pointer in self.graph[node_i]:\n",
    "                self.degrees[pointer] -= 1\n",
    "                if self.degrees[pointer] == 0:\n",
    "                    d.append(pointer)\n",
    "            searched.append(node_i)\n",
    "        return searched\n",
    "        \n",
    "    def add(self, node, to=None):\n",
    "        if not node in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if not to in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "        if len(self.sort()) > len(self.graph):\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a robust DAG, we now have the scheduler to build our intended pipeline. To start, let's add in the DAG class to the Pipeline. Ee then can create a general purpose pipeline with single argument tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self):\n",
    "        scheduled = self.tasks.sort()\n",
    "        completed = {}\n",
    "        for task in scheduled:\n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values:\n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed:\n",
    "                completed[task] = task()\n",
    "        return completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Establishing the data pipeline\n",
    "\n",
    "#### 2.1. Loading the data\n",
    "\n",
    "Because JSON files resemble a key-value dictionary, the goal is to parse the JSON file into a Python `dict` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "        stories = data['stories']\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Filtering the stories\n",
    "\n",
    "Now that we have loaded in all the stories as a list of dict objects, we can now operate on them. Let's start by filtering the list of stories to get the most popular stories of the year.\n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not Ask HN posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    return (story for story in stories if is_popular(story)) #generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Converting to CSV\n",
    "\n",
    "With a reduced set of stories, it's time to write these `dict` objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of our pipeline tasks will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "def build_csv(lines, header=None, file=None):\n",
    "    def open_file(f):\n",
    "        if isinstance(f, str):\n",
    "            f = open(f, 'w')\n",
    "        return f\n",
    "    \n",
    "    file = open_file(file)\n",
    "    if header:\n",
    "        lines = itertools.chain([header], lines)\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    writer.writerows(lines)\n",
    "    file.seek(0)\n",
    "    return file\n",
    "\n",
    "\n",
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'],\n",
    "             datetime.strptime(story['created_at'], \"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "             story['url'],\n",
    "             story['points'],\n",
    "             story['title'])\n",
    "        )\n",
    "    return build_csv(\n",
    "        lines,\n",
    "        header=['objectID', 'created_at', 'url', 'points', 'title'],\n",
    "        file=io.StringIO()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Extracting the title column\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task. To extract them, we can follow these steps:\n",
    "\n",
    "- 1. Import `csv`, and create a `csv.reader()` object from the file object.\n",
    "- 2. Find the index of the `title` in the header.\n",
    "- 3. Iterate the through the reader, and return each item from the reader in the corresponding title index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    idx = header.index('title')\n",
    "    return (line[idx] for line in reader)#generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Cleaning the titles\n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. To clean the titles, we should make sure to lower case the titles, and to remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Creating the word frequency dictionary\n",
    "\n",
    "A word frequency dictionary are key value pairs that connects a word to the number of times it is used in a text. To find actual keywords, we should enforce the word frequency dictionary to not include stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import stop_words\n",
    "\n",
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    word_freq = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words: #ignore empty words\n",
    "                if word not in word_freq:\n",
    "                    word_freq[word] = 1\n",
    "                word_freq[word] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7 Sorting the top words\n",
    "\n",
    "The goal of this task is to output a list of tuples with `(word, frequency)` as the entries sorted from most used, to least most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_keywords(word_freq):\n",
    "    freq_tuple = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq_tuple[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Running the data pipeline\n",
    "\n",
    "We can finally run the pipeline and print the output of the task.\n",
    "\n",
    "We can represent our tasks, and their outputs during a run, by using a dictionary that maps `function: output`. With this dictionary, we can store outputs after task completion so we can use them as inputs for the next tasks that require them.\n",
    "\n",
    "With that output dictionary, we can then choose to see the outputs of any function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "print(output[top_keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final result yielded some interesting keywords. There were terms like `bitcoin` (the cryptocurrency), `heartbleed` (the 2014 hack), and many others. Even though this was a basic natural language processing task, it did provide some interesting insights into conversations from 2014."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
